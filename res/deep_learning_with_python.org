* deep_learning_with_python
:PROPERTIES:
:NOTER_DOCUMENT: deep_learning_with_python.pdf
:END:
*** Skeleton
***** Deep Learning with Python
:PROPERTIES:
:NOTER_PAGE: 1
:END:
***** brief contents
:PROPERTIES:
:NOTER_PAGE: (8 . 0.345345)
:END:
***** contents
:PROPERTIES:
:NOTER_PAGE: (10 . 0.345345)
:END:
***** preface
:PROPERTIES:
:NOTER_PAGE: (18 . 0.345345)
:END:
***** acknowledgments
:PROPERTIES:
:NOTER_PAGE: (20 . 0.345345)
:END:
***** about this book
:PROPERTIES:
:NOTER_PAGE: (21 . 0.345345)
:END:
******* Who should read this book
:PROPERTIES:
:NOTER_PAGE: (22 . 0.075075)
:END:
******* About the code
:PROPERTIES:
:NOTER_PAGE: (22 . 0.521021)
:END:
******* liveBook discussion forum
:PROPERTIES:
:NOTER_PAGE: (22 . 0.833333)
:END:
***** about the author
:PROPERTIES:
:NOTER_PAGE: (24 . 0.345345)
:END:
***** about the cover illustration
:PROPERTIES:
:NOTER_PAGE: (25 . 0.345345)
:END:
***** 1 What is deep learning?
:PROPERTIES:
:NOTER_PAGE: (26 . 0.168168)
:END:
******* 1.1 Artificial intelligence, machine learning, and deep learning
:PROPERTIES:
:NOTER_PAGE: (27 . 0.075075)
:END:
********* 1.1.1 Artificial intelligence
:PROPERTIES:
:NOTER_PAGE: (27 . 0.442943)
:END:
********* 1.1.2 Machine learning
:PROPERTIES:
:NOTER_PAGE: (28 . 0.324324)
:END:
********* 1.1.3 Learning rules and representations from data
:PROPERTIES:
:NOTER_PAGE: (29 . 0.812312)
:END:
********* 1.1.4 The “deep” in “deep learning”
:PROPERTIES:
:NOTER_PAGE: (32 . 0.283784)
:END:
********* 1.1.5 Understanding how deep learning works, in three figures
:PROPERTIES:
:NOTER_PAGE: (33 . 0.795796)
:END:
********* 1.1.6 What deep learning has achieved so far
:PROPERTIES:
:NOTER_PAGE: (35 . 0.678679)
:END:
********* 1.1.7 Don’t believe the short-term hype
:PROPERTIES:
:NOTER_PAGE: (36 . 0.460961)
:END:
********* 1.1.8 The promise of AI
:PROPERTIES:
:NOTER_PAGE: (37 . 0.346847)
:END:
******* 1.2 Before deep learning: A brief history of machine learning
:PROPERTIES:
:NOTER_PAGE: (38 . 0.267267)
:END:
********* 1.2.1 Probabilistic modeling
:PROPERTIES:
:NOTER_PAGE: (38 . 0.623123)
:END:
********* 1.2.2 Early neural networks
:PROPERTIES:
:NOTER_PAGE: (39 . 0.148649)
:END:
********* 1.2.3 Kernel methods
:PROPERTIES:
:NOTER_PAGE: (39 . 0.459459)
:END:
********* 1.2.4 Decision trees, random forests, and gradient boosting machines
:PROPERTIES:
:NOTER_PAGE: (40 . 0.603604)
:END:
********* 1.2.5 Back to neural networks
:PROPERTIES:
:NOTER_PAGE: (41 . 0.405405)
:END:
********* 1.2.6 What makes deep learning different
:PROPERTIES:
:NOTER_PAGE: (42 . 0.267267)
:END:
********* 1.2.7 The modern machine learning landscape
:PROPERTIES:
:NOTER_PAGE: (43 . 0.166667)
:END:
******* 1.3 Why deep learning? Why now?
:PROPERTIES:
:NOTER_PAGE: (45 . 0.267267)
:END:
********* 1.3.1 Hardware
:PROPERTIES:
:NOTER_PAGE: (45 . 0.657658)
:END:
********* 1.3.2 Data
:PROPERTIES:
:NOTER_PAGE: (46 . 0.617117)
:END:
********* 1.3.3 Algorithms
:PROPERTIES:
:NOTER_PAGE: (47 . 0.283784)
:END:
********* 1.3.4 A new wave of investment
:PROPERTIES:
:NOTER_PAGE: (48 . 0.075075)
:END:
********* 1.3.5 The democratization of deep learning
:PROPERTIES:
:NOTER_PAGE: (49 . 0.211712)
:END:
********* 1.3.6 Will it last?
:PROPERTIES:
:NOTER_PAGE: (49 . 0.46997)
:END:
***** 2 The mathematical building blocks of neural networks
:PROPERTIES:
:NOTER_PAGE: (51 . 0.168168)
:END:
******* 2.1 A first look at a neural network
:PROPERTIES:
:NOTER_PAGE: (52 . 0.147147)
:END:
******* 2.2 Data representations for neural networks
:PROPERTIES:
:NOTER_PAGE: (56 . 0.075075)
:END:
********* 2.2.1 Scalars (rank-0 tensors)
:PROPERTIES:
:NOTER_PAGE: (56 . 0.27027)
:END:
********* 2.2.2 Vectors (rank-1 tensors)
:PROPERTIES:
:NOTER_PAGE: (56 . 0.515015)
:END:
********* 2.2.3 Matrices (rank-2 tensors)
:PROPERTIES:
:NOTER_PAGE: (57 . 0.075075)
:END:
********* 2.2.4 Rank-3 and higher-rank tensors
:PROPERTIES:
:NOTER_PAGE: (57 . 0.342342)
:END:
********* 2.2.5 Key attributes
:PROPERTIES:
:NOTER_PAGE: (57 . 0.701201)
:END:
********* 2.2.6 Manipulating tensors in NumPy
:PROPERTIES:
:NOTER_PAGE: (59 . 0.267267)
:END:
********* 2.2.7 The notion of data batches
:PROPERTIES:
:NOTER_PAGE: (60 . 0.075075)
:END:
********* 2.2.8 Real-world examples of data tensors
:PROPERTIES:
:NOTER_PAGE: (60 . 0.483483)
:END:
********* 2.2.9 Vector data
:PROPERTIES:
:NOTER_PAGE: (60 . 0.791291)
:END:
********* 2.2.10 Timeseries data or sequence data
:PROPERTIES:
:NOTER_PAGE: (61 . 0.297297)
:END:
********* 2.2.11 Image data
:PROPERTIES:
:NOTER_PAGE: (62 . 0.075075)
:END:
********* 2.2.12 Video data
:PROPERTIES:
:NOTER_PAGE: (62 . 0.695195)
:END:
******* 2.3 The gears of neural networks: Tensor operations
:PROPERTIES:
:NOTER_PAGE: (63 . 0.166667)
:END:
********* 2.3.1 Element-wise operations
:PROPERTIES:
:NOTER_PAGE: (63 . 0.695195)
:END:
********* 2.3.2 Broadcasting
:PROPERTIES:
:NOTER_PAGE: (65 . 0.148649)
:END:
********* 2.3.3 Tensor product
:PROPERTIES:
:NOTER_PAGE: (66 . 0.366366)
:END:
********* 2.3.4 Tensor reshaping
:PROPERTIES:
:NOTER_PAGE: (68 . 0.509009)
:END:
********* 2.3.5 Geometric interpretation of tensor operations
:PROPERTIES:
:NOTER_PAGE: (69 . 0.292793)
:END:
********* 2.3.6 A geometric interpretation of deep learning
:PROPERTIES:
:NOTER_PAGE: (72 . 0.382883)
:END:
******* 2.4 The engine of neural networks: Gradient-based optimization
:PROPERTIES:
:NOTER_PAGE: (73 . 0.186186)
:END:
********* 2.4.1 What’s a derivative?
:PROPERTIES:
:NOTER_PAGE: (74 . 0.596096)
:END:
********* 2.4.2 Derivative of a tensor operation: The gradient
:PROPERTIES:
:NOTER_PAGE: (76 . 0.075075)
:END:
********* 2.4.3 Stochastic gradient descent
:PROPERTIES:
:NOTER_PAGE: (77 . 0.495495)
:END:
********* 2.4.4 Chaining derivatives: The Backpropagation algorithm
:PROPERTIES:
:NOTER_PAGE: (80 . 0.460961)
:END:
******* 2.5 Looking back at our first example
:PROPERTIES:
:NOTER_PAGE: (86 . 0.252252)
:END:
********* 2.5.1 Reimplementing our first example from scratch in TensorFlow
:PROPERTIES:
:NOTER_PAGE: (88 . 0.075075)
:END:
********* 2.5.2 Running one training step
:PROPERTIES:
:NOTER_PAGE: (89 . 0.78979)
:END:
********* 2.5.3 The full training loop
:PROPERTIES:
:NOTER_PAGE: (90 . 0.783784)
:END:
********* 2.5.4 Evaluating the model
:PROPERTIES:
:NOTER_PAGE: (91 . 0.369369)
:END:
******* Summary
:PROPERTIES:
:NOTER_PAGE: (91 . 0.651652)
:END:
***** 3 Introduction to Keras and TensorFlow
:PROPERTIES:
:NOTER_PAGE: (93 . 0.168168)
:END:
******* 3.1 What’s TensorFlow?
:PROPERTIES:
:NOTER_PAGE: (94 . 0.075075)
:END:
******* 3.2 What’s Keras?
:PROPERTIES:
:NOTER_PAGE: (94 . 0.717718)
:END:
******* 3.3 Keras and TensorFlow: A brief history
:PROPERTIES:
:NOTER_PAGE: (96 . 0.147147)
:END:
******* 3.4 Setting up a deep learning workspace
:PROPERTIES:
:NOTER_PAGE: (96 . 0.732733)
:END:
********* 3.4.1 Jupyter notebooks: The preferred way to run deep learning experiments
:PROPERTIES:
:NOTER_PAGE: (97 . 0.690691)
:END:
********* 3.4.2 Using Colaboratory
:PROPERTIES:
:NOTER_PAGE: (98 . 0.186186)
:END:
******* 3.5 First steps with TensorFlow
:PROPERTIES:
:NOTER_PAGE: (100 . 0.747748)
:END:
********* 3.5.1 Constant tensors and variables
:PROPERTIES:
:NOTER_PAGE: (101 . 0.397898)
:END:
********* 3.5.2 Tensor operations: Doing math in TensorFlow
:PROPERTIES:
:NOTER_PAGE: (103 . 0.363363)
:END:
********* 3.5.3 A second look at the GradientTape API
:PROPERTIES:
:NOTER_PAGE: (103 . 0.672673)
:END:
********* 3.5.4 An end-to-end example: A linear classifier in pure TensorFlow
:PROPERTIES:
:NOTER_PAGE: (104 . 0.755255)
:END:
******* 3.6 Anatomy of a neural network: Understanding core Keras APIs
:PROPERTIES:
:NOTER_PAGE: (109 . 0.126126)
:END:
********* 3.6.1 Layers: The building blocks of deep learning
:PROPERTIES:
:NOTER_PAGE: (109 . 0.282282)
:END:
********* 3.6.2 From layers to models
:PROPERTIES:
:NOTER_PAGE: (112 . 0.249249)
:END:
********* 3.6.3 The “compile” step: Configuring the learning process
:PROPERTIES:
:NOTER_PAGE: (113 . 0.804805)
:END:
********* 3.6.4 Picking a loss function
:PROPERTIES:
:NOTER_PAGE: (115 . 0.513514)
:END:
********* 3.6.5 Understanding the fit() method
:PROPERTIES:
:NOTER_PAGE: (116 . 0.075075)
:END:
********* 3.6.6 Monitoring loss and metrics on validation data
:PROPERTIES:
:NOTER_PAGE: (116 . 0.702703)
:END:
********* 3.6.7 Inference: Using a model after training
:PROPERTIES:
:NOTER_PAGE: (118 . 0.075075)
:END:
******* Summary
:PROPERTIES:
:NOTER_PAGE: (118 . 0.722222)
:END:
***** 4 Getting started with neural networks: Classification and regression
:PROPERTIES:
:NOTER_PAGE: (120 . 0.168168)
:END:
******* 4.1 Classifying movie reviews: A binary classification example
:PROPERTIES:
:NOTER_PAGE: (122 . 0.075075)
:END:
********* 4.1.1 The IMDB dataset
:PROPERTIES:
:NOTER_PAGE: (122 . 0.193694)
:END:
********* 4.1.2 Preparing the data
:PROPERTIES:
:NOTER_PAGE: (123 . 0.268769)
:END:
********* 4.1.3 Building your model
:PROPERTIES:
:NOTER_PAGE: (124 . 0.189189)
:END:
********* 4.1.4 Validating your approach
:PROPERTIES:
:NOTER_PAGE: (127 . 0.252252)
:END:
********* 4.1.5 Using a trained model to generate predictions on new data
:PROPERTIES:
:NOTER_PAGE: (130 . 0.348348)
:END:
********* 4.1.6 Further experiments
:PROPERTIES:
:NOTER_PAGE: (130 . 0.65015)
:END:
********* 4.1.7 Wrapping up
:PROPERTIES:
:NOTER_PAGE: (131 . 0.075075)
:END:
******* 4.2 Classifying newswires: A multiclass classification example
:PROPERTIES:
:NOTER_PAGE: (131 . 0.459459)
:END:
********* 4.2.1 The Reuters dataset
:PROPERTIES:
:NOTER_PAGE: (131 . 0.675676)
:END:
********* 4.2.2 Preparing the data
:PROPERTIES:
:NOTER_PAGE: (132 . 0.587087)
:END:
********* 4.2.3 Building your model
:PROPERTIES:
:NOTER_PAGE: (133 . 0.265766)
:END:
********* 4.2.4 Validating your approach
:PROPERTIES:
:NOTER_PAGE: (134 . 0.246246)
:END:
********* 4.2.5 Generating predictions on new data
:PROPERTIES:
:NOTER_PAGE: (136 . 0.617117)
:END:
********* 4.2.6 A different way to handle the labels and the loss
:PROPERTIES:
:NOTER_PAGE: (137 . 0.157658)
:END:
********* 4.2.7 The importance of having sufficiently large intermediate layers
:PROPERTIES:
:NOTER_PAGE: (137 . 0.490991)
:END:
********* 4.2.8 Further experiments
:PROPERTIES:
:NOTER_PAGE: (138 . 0.148649)
:END:
********* 4.2.9 Wrapping up
:PROPERTIES:
:NOTER_PAGE: (138 . 0.310811)
:END:
******* 4.3 Predicting house prices: A regression example
:PROPERTIES:
:NOTER_PAGE: (138 . 0.696697)
:END:
********* 4.3.1 The Boston housing price dataset
:PROPERTIES:
:NOTER_PAGE: (139 . 0.075075)
:END:
********* 4.3.2 Preparing the data
:PROPERTIES:
:NOTER_PAGE: (139 . 0.668168)
:END:
********* 4.3.3 Building your model
:PROPERTIES:
:NOTER_PAGE: (140 . 0.22973)
:END:
********* 4.3.4 Validating your approach using K-fold validation
:PROPERTIES:
:NOTER_PAGE: (140 . 0.753754)
:END:
********* 4.3.5 Generating predictions on new data
:PROPERTIES:
:NOTER_PAGE: (144 . 0.575075)
:END:
********* 4.3.6 Wrapping up
:PROPERTIES:
:NOTER_PAGE: (144 . 0.807808)
:END:
******* Summary
:PROPERTIES:
:NOTER_PAGE: (145 . 0.268769)
:END:
***** 5 Fundamentals of machine learning
:PROPERTIES:
:NOTER_PAGE: (146 . 0.168168)
:END:
******* 5.1 Generalization: The goal of machine learning
:PROPERTIES:
:NOTER_PAGE: (146 . 0.764264)
:END:
********* 5.1.1 Underfitting and overfitting
:PROPERTIES:
:NOTER_PAGE: (147 . 0.324324)
:END:
********* 5.1.2 The nature of generalization in deep learning
:PROPERTIES:
:NOTER_PAGE: (152 . 0.557057)
:END:
******* 5.2 Evaluating machine learning models
:PROPERTIES:
:NOTER_PAGE: (158 . 0.442943)
:END:
********* 5.2.1 Training, validation, and test sets
:PROPERTIES:
:NOTER_PAGE: (158 . 0.585586)
:END:
********* 5.2.2 Beating a common-sense baseline
:PROPERTIES:
:NOTER_PAGE: (161 . 0.62012)
:END:
********* 5.2.3 Things to keep in mind about model evaluation
:PROPERTIES:
:NOTER_PAGE: (162 . 0.361862)
:END:
******* 5.3 Improving model fit
:PROPERTIES:
:NOTER_PAGE: (163 . 0.075075)
:END:
********* 5.3.1 Tuning key gradient descent parameters
:PROPERTIES:
:NOTER_PAGE: (163 . 0.423423)
:END:
********* 5.3.2 Leveraging better architecture priors
:PROPERTIES:
:NOTER_PAGE: (164 . 0.618619)
:END:
********* 5.3.3 Increasing model capacity
:PROPERTIES:
:NOTER_PAGE: (165 . 0.249249)
:END:
******* 5.4 Improving generalization
:PROPERTIES:
:NOTER_PAGE: (167 . 0.412913)
:END:
********* 5.4.1 Dataset curation
:PROPERTIES:
:NOTER_PAGE: (167 . 0.492492)
:END:
********* 5.4.2 Feature engineering
:PROPERTIES:
:NOTER_PAGE: (168 . 0.075075)
:END:
********* 5.4.3 Using early stopping
:PROPERTIES:
:NOTER_PAGE: (169 . 0.474474)
:END:
********* 5.4.4 Regularizing your model
:PROPERTIES:
:NOTER_PAGE: (170 . 0.075075)
:END:
******* Summary
:PROPERTIES:
:NOTER_PAGE: (177 . 0.237237)
:END:
***** 6 The universal workflow of machine learning
:PROPERTIES:
:NOTER_PAGE: (178 . 0.168168)
:END:
******* 6.1 Define the task
:PROPERTIES:
:NOTER_PAGE: (180 . 0.157658)
:END:
********* 6.1.1 Frame the problem
:PROPERTIES:
:NOTER_PAGE: (180 . 0.297297)
:END:
********* 6.1.2 Collect a dataset
:PROPERTIES:
:NOTER_PAGE: (181 . 0.653153)
:END:
********* 6.1.3 Understand your data
:PROPERTIES:
:NOTER_PAGE: (185 . 0.075075)
:END:
********* 6.1.4 Choose a measure of success
:PROPERTIES:
:NOTER_PAGE: (185 . 0.576577)
:END:
******* 6.2 Develop a model
:PROPERTIES:
:NOTER_PAGE: (186 . 0.075075)
:END:
********* 6.2.1 Prepare the data
:PROPERTIES:
:NOTER_PAGE: (186 . 0.274775)
:END:
********* 6.2.2 Choose an evaluation protocol
:PROPERTIES:
:NOTER_PAGE: (187 . 0.792793)
:END:
********* 6.2.3 Beat a baseline
:PROPERTIES:
:NOTER_PAGE: (188 . 0.351351)
:END:
********* 6.2.4 Scale up: Develop a model that overfits
:PROPERTIES:
:NOTER_PAGE: (189 . 0.557057)
:END:
********* 6.2.5 Regularize and tune your model
:PROPERTIES:
:NOTER_PAGE: (190 . 0.075075)
:END:
******* 6.3 Deploy the model
:PROPERTIES:
:NOTER_PAGE: (190 . 0.704204)
:END:
********* 6.3.1 Explain your work to stakeholders and set expectations
:PROPERTIES:
:NOTER_PAGE: (190 . 0.785285)
:END:
********* 6.3.2 Ship an inference model
:PROPERTIES:
:NOTER_PAGE: (191 . 0.483483)
:END:
********* 6.3.3 Monitor your model in the wild
:PROPERTIES:
:NOTER_PAGE: (194 . 0.474474)
:END:
********* 6.3.4 Maintain your model
:PROPERTIES:
:NOTER_PAGE: (195 . 0.153153)
:END:
******* Summary
:PROPERTIES:
:NOTER_PAGE: (195 . 0.581081)
:END:
***** 7 Working with Keras: A deep dive
:PROPERTIES:
:NOTER_PAGE: (197 . 0.168168)
:END:
******* 7.1 A spectrum of workflows
:PROPERTIES:
:NOTER_PAGE: (198 . 0.127628)
:END:
******* 7.2 Different ways to build Keras models
:PROPERTIES:
:NOTER_PAGE: (198 . 0.43994)
:END:
********* 7.2.1 The Sequential model
:PROPERTIES:
:NOTER_PAGE: (199 . 0.075075)
:END:
********* 7.2.2 The Functional API
:PROPERTIES:
:NOTER_PAGE: (201 . 0.605105)
:END:
********* 7.2.3 Subclassing the Model class
:PROPERTIES:
:NOTER_PAGE: (207 . 0.355856)
:END:
********* 7.2.4 Mixing and matching different components
:PROPERTIES:
:NOTER_PAGE: (209 . 0.303303)
:END:
********* 7.2.5 Remember: Use the right tool for the job
:PROPERTIES:
:NOTER_PAGE: (210 . 0.274775)
:END:
******* 7.3 Using built-in training and evaluation loops
:PROPERTIES:
:NOTER_PAGE: (210 . 0.585586)
:END:
********* 7.3.1 Writing your own metrics
:PROPERTIES:
:NOTER_PAGE: (211 . 0.540541)
:END:
********* 7.3.2 Using callbacks
:PROPERTIES:
:NOTER_PAGE: (212 . 0.779279)
:END:
********* 7.3.3 Writing your own callbacks
:PROPERTIES:
:NOTER_PAGE: (214 . 0.615616)
:END:
********* 7.3.4 Monitoring and visualization with TensorBoard
:PROPERTIES:
:NOTER_PAGE: (215 . 0.735736)
:END:
******* 7.4 Writing your own training and evaluation loops
:PROPERTIES:
:NOTER_PAGE: (217 . 0.71021)
:END:
********* 7.4.1 Training versus inference
:PROPERTIES:
:NOTER_PAGE: (219 . 0.186186)
:END:
********* 7.4.2 Low-level usage of metrics
:PROPERTIES:
:NOTER_PAGE: (220 . 0.123123)
:END:
********* 7.4.3 A complete training and evaluation loop
:PROPERTIES:
:NOTER_PAGE: (220 . 0.546547)
:END:
********* 7.4.4 Make it fast with tf.function
:PROPERTIES:
:NOTER_PAGE: (222 . 0.388889)
:END:
********* 7.4.5 Leveraging fit() with a custom training loop
:PROPERTIES:
:NOTER_PAGE: (223 . 0.298799)
:END:
******* Summary
:PROPERTIES:
:NOTER_PAGE: (225 . 0.426426)
:END:
***** 8 Introduction to deep learning for computer vision
:PROPERTIES:
:NOTER_PAGE: (226 . 0.168168)
:END:
******* 8.1 Introduction to convnets
:PROPERTIES:
:NOTER_PAGE: (227 . 0.463964)
:END:
********* 8.1.1 The convolution operation
:PROPERTIES:
:NOTER_PAGE: (229 . 0.53003)
:END:

This key characteristic gives convnets two interesting properties:

 The patterns they learn are translation-invariant. After learning a certain
pattern in the lower-right corner of a picture, a convnet can recognize it
anywhere: for example, in the upper-left corner. A densely connected model would
have to learn the pattern anew if it appeared at a new location. This makes
convnets data-efficient when processing images (because the visual world is
fundamentally translation-invariant): they need fewer training samples to learn
representations that have generalization power.

 They can learn spatial hierarchies of patterns. A first convolution layer will
learn small local patterns such as edges, a second convolution layer will learn
larger patterns made of the features of the first layers, and so on (see figure
8.2). This allows convnets to efficiently learn increasingly complex and
abstract visual con- cepts, because the visual world is fundamentally spatially
hierarchical. “cat” Figure 8.2 The visual world forms a spatial hierarchy of
visual modules: elementary lines or textures combine into simple objects such as
eyes or ears, which combine into high-level concepts such as “cat.”
:PROPERTIES:
:NOTER_PAGE: (230 . 0.08438287153652392)
:END:

********* 8.1.2 The max-pooling operation
:PROPERTIES:
:NOTER_PAGE: (234 . 0.60961)
:END:
********* Notes for page 235
:PROPERTIES:
:NOTER_PAGE: 235
:END:
******* 8.2 Training a convnet from scratch on a small dataset
:PROPERTIES:
:NOTER_PAGE: (236 . 0.503003)
:END:
********* 8.2.1 The relevance of deep learning for small-data problems
:PROPERTIES:
:NOTER_PAGE: (237 . 0.127628)
:END:
********* 8.2.2 Downloading the data
:PROPERTIES:
:NOTER_PAGE: (237 . 0.478979)
:END:
********* 8.2.3 Building the model
:PROPERTIES:
:NOTER_PAGE: (240 . 0.45045)
:END:
********* 8.2.4 Data preprocessing
:PROPERTIES:
:NOTER_PAGE: (242 . 0.261261)
:END:
********* 8.2.5 Using data augmentation
:PROPERTIES:
:NOTER_PAGE: (246 . 0.325826)
:END:
******* 8.3 Leveraging a pretrained model
:PROPERTIES:
:NOTER_PAGE: (249 . 0.719219)
:END:
********* 8.3.1 Feature extraction with a pretrained model
:PROPERTIES:
:NOTER_PAGE: (250 . 0.459459)
:END:
********* 8.3.2 Fine-tuning a pretrained model
:PROPERTIES:
:NOTER_PAGE: (259 . 0.075075)
:END:
******* Summary
:PROPERTIES:
:NOTER_PAGE: (262 . 0.303303)
:END:
***** 9 Advanced deep learning for computer vision
:PROPERTIES:
:NOTER_PAGE: (263 . 0.168168)
:END:
******* 9.1 Three essential computer vision tasks
:PROPERTIES:
:NOTER_PAGE: (263 . 0.776276)
:END:
******* 9.2 An image segmentation example
:PROPERTIES:
:NOTER_PAGE: (265 . 0.385886)
:END:
******* 9.3 Modern convnet architecture patterns
:PROPERTIES:
:NOTER_PAGE: (273 . 0.283784)
:END:
********* 9.3.1 Modularity, hierarchy, and reuse
:PROPERTIES:
:NOTER_PAGE: (274 . 0.127628)
:END:
********* 9.3.2 Residual connections
:PROPERTIES:
:NOTER_PAGE: (276 . 0.698198)
:END:
********* 9.3.3 Batch normalization
:PROPERTIES:
:NOTER_PAGE: (280 . 0.375375)
:END:
********* 9.3.4 Depthwise separable convolutions
:PROPERTIES:
:NOTER_PAGE: (282 . 0.414414)
:END:
********* 9.3.5 Putting it together: A mini Xception-like model
:PROPERTIES:
:NOTER_PAGE: (284 . 0.542042)
:END:
******* 9.4 Interpreting what convnets learn
:PROPERTIES:
:NOTER_PAGE: (286 . 0.42042)
:END:
********* 9.4.1 Visualizing intermediate activations
:PROPERTIES:
:NOTER_PAGE: (287 . 0.217718)
:END:
********* 9.4.2 Visualizing convnet filters
:PROPERTIES:
:NOTER_PAGE: (293 . 0.127628)
:END:
********* 9.4.3 Visualizing heatmaps of class activation
:PROPERTIES:
:NOTER_PAGE: (298 . 0.746246)
:END:
******* Summary
:PROPERTIES:
:NOTER_PAGE: (304 . 0.220721)
:END:
***** 10 Deep learning for timeseries
:PROPERTIES:
:NOTER_PAGE: (305 . 0.168168)
:END:
******* 10.1 Different kinds of timeseries tasks
:PROPERTIES:
:NOTER_PAGE: (305 . 0.633634)
:END:
******* 10.2 A temperature-forecasting example
:PROPERTIES:
:NOTER_PAGE: (306 . 0.734234)
:END:
********* 10.2.1 Preparing the data
:PROPERTIES:
:NOTER_PAGE: (310 . 0.184685)
:END:
********* 10.2.2 A common-sense, non-machine learning baseline
:PROPERTIES:
:NOTER_PAGE: (313 . 0.075075)
:END:
********* 10.2.3 Let’s try a basic machine learning model
:PROPERTIES:
:NOTER_PAGE: (314 . 0.075075)
:END:
********* 10.2.4 Let’s try a 1D convolutional model
:PROPERTIES:
:NOTER_PAGE: (315 . 0.764264)
:END:
********* 10.2.5 A first recurrent baseline
:PROPERTIES:
:NOTER_PAGE: (317 . 0.423423)
:END:
******* 10.3 Understanding recurrent neural networks
:PROPERTIES:
:NOTER_PAGE: (318 . 0.659159)
:END:
********* 10.3.1 A recurrent layer in Keras
:PROPERTIES:
:NOTER_PAGE: (321 . 0.216216)
:END:
******* 10.4 Advanced use of recurrent neural networks
:PROPERTIES:
:NOTER_PAGE: (325 . 0.400901)
:END:
********* 10.4.1 Using recurrent dropout to fight overfitting
:PROPERTIES:
:NOTER_PAGE: (325 . 0.765766)
:END:
********* 10.4.2 Stacking recurrent layers
:PROPERTIES:
:NOTER_PAGE: (328 . 0.27027)
:END:
********* 10.4.3 Using bidirectional RNNs
:PROPERTIES:
:NOTER_PAGE: (329 . 0.696697)
:END:
********* 10.4.4 Going even further
:PROPERTIES:
:NOTER_PAGE: (332 . 0.382883)
:END:
******* Summary
:PROPERTIES:
:NOTER_PAGE: (333 . 0.385886)
:END:
***** 11 Deep learning for text
:PROPERTIES:
:NOTER_PAGE: (334 . 0.168168)
:END:
******* 11.1 Natural language processing: The bird’s eye view
:PROPERTIES:
:NOTER_PAGE: (334 . 0.633634)
:END:
******* 11.2 Preparing text data
:PROPERTIES:
:NOTER_PAGE: (336 . 0.596096)
:END:
********* 11.2.1 Text standardization
:PROPERTIES:
:NOTER_PAGE: (337 . 0.498498)
:END:
********* 11.2.2 Text splitting (tokenization)
:PROPERTIES:
:NOTER_PAGE: (338 . 0.454955)
:END:
********* 11.2.3 Vocabulary indexing
:PROPERTIES:
:NOTER_PAGE: (339 . 0.608108)
:END:
********* 11.2.4 Using the TextVectorization layer
:PROPERTIES:
:NOTER_PAGE: (341 . 0.075075)
:END:
******* 11.3 Two approaches for representing groups of words: Sets and sequences
:PROPERTIES:
:NOTER_PAGE: (344 . 0.48048)
:END:
********* 11.3.1 Preparing the IMDB movie reviews data
:PROPERTIES:
:NOTER_PAGE: (345 . 0.342342)
:END:
********* 11.3.2 Processing words as a set: The bag-of-words approach
:PROPERTIES:
:NOTER_PAGE: (347 . 0.168168)
:END:
********* 11.3.3 Processing words as a sequence: The sequence model approach
:PROPERTIES:
:NOTER_PAGE: (352 . 0.626126)
:END:
******* 11.4 The Transformer architecture
:PROPERTIES:
:NOTER_PAGE: (361 . 0.636637)
:END:
********* 11.4.1 Understanding self-attention
:PROPERTIES:
:NOTER_PAGE: (362 . 0.126126)
:END:
********* 11.4.2 Multi-head attention
:PROPERTIES:
:NOTER_PAGE: (366 . 0.600601)
:END:
********* 11.4.3 The Transformer encoder
:PROPERTIES:
:NOTER_PAGE: (367 . 0.627628)
:END:
********* 11.4.4 When to use sequence models over bag-of-words models
:PROPERTIES:
:NOTER_PAGE: (374 . 0.075075)
:END:
******* 11.5 Beyond text classification: Sequence-to-sequence learning
:PROPERTIES:
:NOTER_PAGE: (375 . 0.267267)
:END:
********* 11.5.1 A machine translation example
:PROPERTIES:
:NOTER_PAGE: (376 . 0.608108)
:END:
********* 11.5.2 Sequence-to-sequence learning with RNNs
:PROPERTIES:
:NOTER_PAGE: (379 . 0.207207)
:END:
********* 11.5.3 Sequence-to-sequence learning with Transformer
:PROPERTIES:
:NOTER_PAGE: (383 . 0.18018)
:END:
******* Summary
:PROPERTIES:
:NOTER_PAGE: (388 . 0.40991)
:END:
***** 12 Generative deep learning
:PROPERTIES:
:NOTER_PAGE: (389 . 0.168168)
:END:
******* 12.1 Text generation
:PROPERTIES:
:NOTER_PAGE: (391 . 0.190691)
:END:
********* 12.1.1 A brief history of generative deep learning for sequence generation
:PROPERTIES:
:NOTER_PAGE: (391 . 0.451952)
:END:
********* 12.1.2 How do you generate sequence data?
:PROPERTIES:
:NOTER_PAGE: (392 . 0.382883)
:END:
********* 12.1.3 The importance of the sampling strategy
:PROPERTIES:
:NOTER_PAGE: (393 . 0.075075)
:END:
********* 12.1.4 Implementing text generation with Keras
:PROPERTIES:
:NOTER_PAGE: (394 . 0.744745)
:END:
********* 12.1.5 A text-generation callback with variable-temperature sampling
:PROPERTIES:
:NOTER_PAGE: (397 . 0.821321)
:END:
********* 12.1.6 Wrapping up
:PROPERTIES:
:NOTER_PAGE: (401 . 0.075075)
:END:
******* 12.2 DeepDream
:PROPERTIES:
:NOTER_PAGE: (401 . 0.274775)
:END:
********* 12.2.1 Implementing DeepDream in Keras
:PROPERTIES:
:NOTER_PAGE: (402 . 0.378378)
:END:
********* 12.2.2 Wrapping up
:PROPERTIES:
:NOTER_PAGE: (408 . 0.075075)
:END:
******* 12.3 Neural style transfer
:PROPERTIES:
:NOTER_PAGE: (408 . 0.234234)
:END:
********* 12.3.1 The content loss
:PROPERTIES:
:NOTER_PAGE: (409 . 0.45045)
:END:
********* 12.3.2 The style loss
:PROPERTIES:
:NOTER_PAGE: (409 . 0.749249)
:END:
********* 12.3.3 Neural style transfer in Keras
:PROPERTIES:
:NOTER_PAGE: (410 . 0.436937)
:END:
********* 12.3.4 Wrapping up
:PROPERTIES:
:NOTER_PAGE: (416 . 0.207207)
:END:
******* 12.4 Generating images with variational autoencoders
:PROPERTIES:
:NOTER_PAGE: (416 . 0.406907)
:END:
********* 12.4.1 Sampling from latent spaces of images
:PROPERTIES:
:NOTER_PAGE: (416 . 0.662162)
:END:
********* 12.4.2 Concept vectors for image editing
:PROPERTIES:
:NOTER_PAGE: (418 . 0.075075)
:END:
********* 12.4.3 Variational autoencoders
:PROPERTIES:
:NOTER_PAGE: (418 . 0.672673)
:END:
********* 12.4.4 Implementing a VAE with Keras
:PROPERTIES:
:NOTER_PAGE: (421 . 0.075075)
:END:
********* 12.4.5 Wrapping up
:PROPERTIES:
:NOTER_PAGE: (426 . 0.127628)
:END:
******* 12.5 Introduction to generative adversarial networks
:PROPERTIES:
:NOTER_PAGE: (426 . 0.468468)
:END:
********* 12.5.1 A schematic GAN implementation
:PROPERTIES:
:NOTER_PAGE: (427 . 0.762763)
:END:
********* 12.5.2 A bag of tricks
:PROPERTIES:
:NOTER_PAGE: (428 . 0.765766)
:END:
********* 12.5.3 Getting our hands on the CelebA dataset
:PROPERTIES:
:NOTER_PAGE: (429 . 0.758258)
:END:
********* 12.5.4 The discriminator
:PROPERTIES:
:NOTER_PAGE: (430 . 0.711712)
:END:
********* 12.5.5 The generator
:PROPERTIES:
:NOTER_PAGE: (432 . 0.075075)
:END:
********* 12.5.6 The adversarial network
:PROPERTIES:
:NOTER_PAGE: (433 . 0.075075)
:END:
********* 12.5.7 Wrapping up
:PROPERTIES:
:NOTER_PAGE: (435 . 0.822823)
:END:
******* Summary
:PROPERTIES:
:NOTER_PAGE: (436 . 0.352853)
:END:
***** 13 Best practices for the real world
:PROPERTIES:
:NOTER_PAGE: (437 . 0.168168)
:END:
******* 13.1 Getting the most out of your models
:PROPERTIES:
:NOTER_PAGE: (438 . 0.171171)
:END:
********* 13.1.1 Hyperparameter optimization
:PROPERTIES:
:NOTER_PAGE: (438 . 0.295796)
:END:
********* 13.1.2 Model ensembling
:PROPERTIES:
:NOTER_PAGE: (445 . 0.127628)
:END:
******* 13.2 Scaling-up model training
:PROPERTIES:
:NOTER_PAGE: (446 . 0.771772)
:END:
********* 13.2.1 Speeding up training on GPU with mixed precision
:PROPERTIES:
:NOTER_PAGE: (447 . 0.507508)
:END:
********* 13.2.2 Multi-GPU training
:PROPERTIES:
:NOTER_PAGE: (450 . 0.423423)
:END:
********* 13.2.3 TPU training
:PROPERTIES:
:NOTER_PAGE: (453 . 0.075075)
:END:
******* Summary
:PROPERTIES:
:NOTER_PAGE: (455 . 0.37988)
:END:
***** 14 Conclusions
:PROPERTIES:
:NOTER_PAGE: (456 . 0.168168)
:END:
******* 14.1 Key concepts in review
:PROPERTIES:
:NOTER_PAGE: (457 . 0.147147)
:END:
********* 14.1.1 Various approaches to AI
:PROPERTIES:
:NOTER_PAGE: (457 . 0.225225)
:END:
********* 14.1.2 What makes deep learning special within the field of machine learning
:PROPERTIES:
:NOTER_PAGE: (457 . 0.72973)
:END:
********* 14.1.3 How to think about deep learning
:PROPERTIES:
:NOTER_PAGE: (458 . 0.518018)
:END:
********* 14.1.4 Key enabling technologies
:PROPERTIES:
:NOTER_PAGE: (459 . 0.675676)
:END:
********* 14.1.5 The universal machine learning workflow
:PROPERTIES:
:NOTER_PAGE: (460 . 0.45045)
:END:
********* 14.1.6 Key network architectures
:PROPERTIES:
:NOTER_PAGE: (461 . 0.384384)
:END:
********* 14.1.7 The space of possibilities
:PROPERTIES:
:NOTER_PAGE: (465 . 0.644144)
:END:
******* 14.2 The limitations of deep learning
:PROPERTIES:
:NOTER_PAGE: (467 . 0.358859)
:END:
********* 14.2.1 The risk of anthropomorphizing machine learning models
:PROPERTIES:
:NOTER_PAGE: (468 . 0.075075)
:END:
********* 14.2.2 Automatons vs. intelligent agents
:PROPERTIES:
:NOTER_PAGE: (470 . 0.207207)
:END:
********* 14.2.3 Local generalization vs. extreme generalization
:PROPERTIES:
:NOTER_PAGE: (471 . 0.603604)
:END:
********* 14.2.4 The purpose of intelligence
:PROPERTIES:
:NOTER_PAGE: (473 . 0.648649)
:END:
********* 14.2.5 Climbing the spectrum of generalization
:PROPERTIES:
:NOTER_PAGE: (474 . 0.445946)
:END:
******* 14.3 Setting the course toward greater generality in AI
:PROPERTIES:
:NOTER_PAGE: (475 . 0.557057)
:END:
********* 14.3.1 On the importance of setting the right objective: The shortcut rule
:PROPERTIES:
:NOTER_PAGE: (475 . 0.675676)
:END:
********* 14.3.2 A new target
:PROPERTIES:
:NOTER_PAGE: (477 . 0.490991)
:END:
******* 14.4 Implementing intelligence: The missing ingredients
:PROPERTIES:
:NOTER_PAGE: (479 . 0.22973)
:END:
********* 14.4.1 Intelligence as sensitivity to abstract analogies
:PROPERTIES:
:NOTER_PAGE: (479 . 0.333333)
:END:
********* 14.4.2 The two poles of abstraction
:PROPERTIES:
:NOTER_PAGE: (480 . 0.660661)
:END:
********* 14.4.3 The missing half of the picture
:PROPERTIES:
:NOTER_PAGE: (483 . 0.683183)
:END:
******* 14.5 The future of deep learning
:PROPERTIES:
:NOTER_PAGE: (484 . 0.615616)
:END:
********* 14.5.1 Models as programs
:PROPERTIES:
:NOTER_PAGE: (485 . 0.256757)
:END:
********* 14.5.2 Blending together deep learning and program synthesis
:PROPERTIES:
:NOTER_PAGE: (486 . 0.366366)
:END:
********* 14.5.3 Lifelong learning and modular subroutine reuse
:PROPERTIES:
:NOTER_PAGE: (488 . 0.674174)
:END:
********* 14.5.4 The long-term vision
:PROPERTIES:
:NOTER_PAGE: (490 . 0.127628)
:END:
******* 14.6 Staying up to date in a fast-moving field
:PROPERTIES:
:NOTER_PAGE: (491 . 0.075075)
:END:
********* 14.6.1 Practice on real-world problems using Kaggle
:PROPERTIES:
:NOTER_PAGE: (491 . 0.28979)
:END:
********* 14.6.2 Read about the latest developments on arXiv
:PROPERTIES:
:NOTER_PAGE: (491 . 0.563063)
:END:
********* 14.6.3 Explore the Keras ecosystem
:PROPERTIES:
:NOTER_PAGE: (492 . 0.127628)
:END:
******* Final words
:PROPERTIES:
:NOTER_PAGE: (492 . 0.372372)
:END:
***** index
:PROPERTIES:
:NOTER_PAGE: (494 . 0.201201)
:END:
******* Symbols
:PROPERTIES:
:NOTER_PAGE: (494 . 0.31982)
:END:
******* A
:PROPERTIES:
:NOTER_PAGE: (494 . 0.37988)
:END:
******* B
:PROPERTIES:
:NOTER_PAGE: (494 . 0.62012)
:END:
******* C
:PROPERTIES:
:NOTER_PAGE: (495 . 0.076577)
:END:
******* D
:PROPERTIES:
:NOTER_PAGE: (495 . 0.602102)
:END:
******* E
:PROPERTIES:
:NOTER_PAGE: (496 . 0.181682)
:END:
******* F
:PROPERTIES:
:NOTER_PAGE: (496 . 0.436937)
:END:
******* G
:PROPERTIES:
:NOTER_PAGE: (496 . 0.211712)
:END:
******* H
:PROPERTIES:
:NOTER_PAGE: (497 . 0.602102)
:END:
******* I
:PROPERTIES:
:NOTER_PAGE: (497 . 0.226727)
:END:
******* J
:PROPERTIES:
:NOTER_PAGE: (497 . 0.647147)
:END:
******* K
:PROPERTIES:
:NOTER_PAGE: (497 . 0.722222)
:END:
******* L
:PROPERTIES:
:NOTER_PAGE: (498 . 0.286787)
:END:
******* M
:PROPERTIES:
:NOTER_PAGE: (498 . 0.136637)
:END:
******* N
:PROPERTIES:
:NOTER_PAGE: (500 . 0.436937)
:END:
******* O
:PROPERTIES:
:NOTER_PAGE: (500 . 0.451952)
:END:
******* P
:PROPERTIES:
:NOTER_PAGE: (500 . 0.767267)
:END:
******* Q
:PROPERTIES:
:NOTER_PAGE: (501 . 0.496997)
:END:
******* R
:PROPERTIES:
:NOTER_PAGE: (501 . 0.557057)
:END:
******* S
:PROPERTIES:
:NOTER_PAGE: (501 . 0.557057)
:END:
******* T
:PROPERTIES:
:NOTER_PAGE: (502 . 0.361862)
:END:
******* U
:PROPERTIES:
:NOTER_PAGE: (503 . 0.136637)
:END:
******* V
:PROPERTIES:
:NOTER_PAGE: (503 . 0.286787)
:END:
******* W
:PROPERTIES:
:NOTER_PAGE: (503 . 0.211712)
:END:
******* X
:PROPERTIES:
:NOTER_PAGE: (503 . 0.406907)
:END:
